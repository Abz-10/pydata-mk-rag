{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13496faa",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots with LangChain and Gemini\n",
    "\n",
    "In this example, we'll build an AI chatbot from start to finish. We will use LangChain, Google's Gemini models, and the Chroma vector database to build a chatbot capable of learning from external documents using **R**etrieval **A**ugmented **G**eneration (RAG).\n",
    "\n",
    "We will start by creating a simple conversational agent, see where it fails, and then enhance it with a knowledge base built from a PDF document. By the end of this notebook, you'll have a functioning RAG pipeline that can hold a conversation and provide informative, source-backed responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd57871",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before we start, we need to install the necessary Python libraries. Here's a quick overview of their roles:\n",
    "\n",
    "-   **dotenv**: Helps manage environment variables, like API keys.\n",
    "-   **sentence-transformers**: Provides state-of-the-art models for creating text embeddings locally.\n",
    "-   **langchain**: The core framework we'll use to \"chain\" components together.\n",
    "-   **langchain-community** & **langchain-google-genai**: Provide integrations for various LLMs and tools.\n",
    "-   **langchain-chroma**: The integration for the Chroma vector database.\n",
    "-   **langchain-huggingface**: Provides integrations for Hugging Face models, including our embedding model.\n",
    "-   **pypdf**: A library to load and parse PDF files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9af126",
   "metadata": {},
   "source": [
    "#### You can install these libraries using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    dotenv \\\n",
    "    sentence-transformers \\\n",
    "    langchain \\\n",
    "    langchain-community \\\n",
    "    langchain-chroma \\\n",
    "    langchain-google-genai \\\n",
    "    langchain-huggingface \\\n",
    "    pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9c0db",
   "metadata": {},
   "source": [
    "### Building a Chatbot (without RAG)\n",
    "\n",
    "We'll begin by creating a simple chatbot without any retrieval augmentation. To do this, we'll initialize a `ChatGoogleGenerativeAI` object from LangChain. This requires a [Google API key](https://aistudio.google.com/apikey). The `getpass` function provides a secure way to enter your key if it's not already set as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9587a59",
   "metadata": {},
   "source": [
    "We can test that the model is working correctly by invoking it with a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c448c276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Where Python's serpent coils around,\\nAnd data's mysteries are found,\\nPyData gathers, bright and keen,\\nA vibrant, knowledge-sharing scene.\\n\\nFrom Pandas frames to plots so grand,\\nWith NumPy's might in every hand,\\nWe parse, predict, and visualize,\\nNew patterns rising to our eyes.\\n\\nA community, strong and true,\\nWhere insights bloom and friendships too.\\nFor open source, a common goal,\\nMaking data make us whole.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--a66ea015-4e00-472c-a85c-81e9fafd075b-0', usage_metadata={'input_tokens': 8, 'output_tokens': 1180, 'total_tokens': 1188, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1077}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or getpass(\n",
    "    \"Enter your Google API key: \"\n",
    ")\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    model='gemini-2.5-flash'\n",
    ")\n",
    "\n",
    "chat.invoke(\"Write a short poem about pydata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666caef",
   "metadata": {},
   "source": [
    "### Structuring a Conversation\n",
    "\n",
    "Chats with generative models are structured as a sequence of messages, each with a specific role. This allows the model to understand the context of the conversation.\n",
    "\n",
    "In LangChain, we use message objects to represent this structure:\n",
    "*   `SystemMessage`: Sets the overall behavior and personality of the assistant (e.g., \"You are a helpful assistant.\").\n",
    "*   `HumanMessage`: Represents a prompt from the user.\n",
    "*   `AIMessage`: Represents a response from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33208d",
   "metadata": {},
   "source": [
    "Let's create a short conversation history to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d8097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand what an LLM is.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2befc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An LLM, or **Large Language Model**, is a type of artificial intelligence (AI) program designed to understand, generate, and process human language.\\n\\nLet\\'s break down each part of the name:\\n\\n1.  **Large:** This refers to two main things:\\n    *   **Massive Datasets:** LLMs are trained on enormous amounts of text and code data – billions, even trillions, of words from books, articles, websites, conversations, and more. This vast exposure helps them learn the nuances of language.\\n    *   **Billions of Parameters:** They have an incredibly large number of \"parameters,\" which are like the internal knobs and dials that the model adjusts during training to learn patterns and relationships in the data. More parameters generally mean a more complex and capable model.\\n\\n2.  **Language:** This indicates their primary focus: human language. They are designed to work with, understand, and produce text, whether it\\'s written or spoken (after being converted to text).\\n\\n3.  **Model:** This means it\\'s a computational framework or algorithm that has been trained to perform a specific task – in this case, understanding and generating language. It\\'s not a conscious entity, but a sophisticated pattern-matching and prediction system.\\n\\n**How do they work (in a simplified way)?**\\n\\nAt their core, LLMs are trained to **predict the next word** in a sequence. By doing this millions of times across a vast dataset, they learn:\\n*   **Grammar and Syntax:** How words fit together correctly.\\n*   **Facts and Knowledge:** Information embedded within the text they\\'ve read.\\n*   **Reasoning and Logic:** Implicit connections and patterns in how information is presented.\\n*   **Style and Tone:** Different ways language is used for various purposes.\\n\\nWhen you give an LLM a \"prompt\" (a question or instruction), it uses its learned patterns to generate a highly probable and coherent response, word by word, based on the context you\\'ve provided.\\n\\n**Key Capabilities of LLMs:**\\n\\n*   **Generating Text:** Writing articles, stories, poems, code, emails, marketing copy, etc.\\n*   **Answering Questions:** Providing information on a wide range of topics.\\n*   **Summarizing Information:** Condensing long texts into shorter versions.\\n*   **Translating Languages:** Converting text from one language to another.\\n*   **Brainstorming Ideas:** Helping with creative tasks or problem-solving.\\n*   **Code Generation and Debugging:** Writing or fixing programming code.\\n*   **Conversational AI:** Powering chatbots and virtual assistants.\\n\\n**Examples of LLMs you might have heard of:**\\n\\n*   **GPT (Generative Pre-trained Transformer)** series by OpenAI (like the one powering ChatGPT)\\n*   **LaMDA / Gemini** by Google\\n*   **Llama** by Meta\\n*   **Claude** by Anthropic\\n\\nIn essence, LLMs are powerful AI systems that have revolutionized how we interact with information and create content, leveraging their immense training to process and generate human-like text with remarkable fluency and coherence.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat.invoke(messages)\n",
    "res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206eadd",
   "metadata": {},
   "source": [
    "### Maintaining Conversational Context\n",
    "\n",
    "To have a real conversation, the chatbot needs to remember what has already been said. We can achieve this by appending the latest AI response and the new user prompt to our list of messages before making the next call.\n",
    "\n",
    "This simple loop of \"prompt -> get response -> append history\" forms the basis of any stateful chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyData members, with their strong foundation in Python, data analysis, machine learning, and scientific computing, are uniquely positioned to leverage LLMs in highly effective ways. LLMs can act as powerful assistants, knowledge bases, and even collaborators, significantly boosting productivity and enabling new kinds of analyses.\n",
      "\n",
      "Here are numerous ways PyData members can use LLMs:\n",
      "\n",
      "### 1. Code Assistance & Development\n",
      "\n",
      "*   **Code Generation:**\n",
      "    *   **Boilerplate Code:** Quickly generate functions, classes, or scripts for common tasks (e.g., a function to load data from a CSV, a simple Flask API endpoint, a basic `matplotlib` plot).\n",
      "    *   **Specific Library Usage:** Ask for examples of how to use a particular function in Pandas, NumPy, Scikit-learn, or PyTorch for a specific task. \"Show me how to perform a group-by aggregation with multiple columns in Pandas.\"\n",
      "    *   **SQL/Query Generation:** Generate complex SQL queries based on natural language descriptions, which is invaluable when working with databases.\n",
      "*   **Code Explanation & Understanding:**\n",
      "    *   **Demystifying Code:** Get explanations for complex or unfamiliar code snippets, including docstring generation. \"Explain what this `lambda` function does in this Pandas `apply` call.\"\n",
      "    *   **Refactoring Suggestions:** Ask for ways to make code more Pythonic, efficient, or readable.\n",
      "*   **Debugging & Error Resolution:**\n",
      "    *   **Error Message Interpretation:** Paste traceback errors and ask for potential causes and solutions. \"I'm getting this `KeyError`, what could be wrong with my dictionary access?\"\n",
      "    *   **Debugging Strategies:** Get suggestions on how to debug a particular piece of code.\n",
      "*   **Test Generation:**\n",
      "    *   Generate unit tests for existing functions or classes, saving time in ensuring code quality.\n",
      "\n",
      "### 2. Data Exploration & Preprocessing\n",
      "\n",
      "*   **Data Cleaning & Transformation Suggestions:**\n",
      "    *   \"How can I handle missing values in a time series dataset?\"\n",
      "    *   \"Suggest ways to normalize text data in a Pandas DataFrame column.\"\n",
      "    *   \"Give me Python code to convert this messy date string column into datetime objects.\"\n",
      "*   **Feature Engineering Ideas:**\n",
      "    *   Brainstorm potential new features from existing data, especially for text or categorical data. \"What features could I extract from a 'product description' column for a recommendation system?\"\n",
      "*   **Data Summarization & Description:**\n",
      "    *   Ask for code to generate descriptive statistics or visualize data distributions. \"Show me how to plot a histogram of 'age' and a scatter plot of 'price' vs 'quantity' using Seaborn.\"\n",
      "\n",
      "### 3. Machine Learning Workflow Enhancement\n",
      "\n",
      "*   **Model Selection & Hyperparameter Tuning:**\n",
      "    *   Get recommendations for suitable ML models based on data characteristics and problem type. \"Which classification algorithm is best for a small, imbalanced dataset?\"\n",
      "    *   Suggest common hyperparameter ranges or tuning strategies for specific models.\n",
      "*   **Interpreting Model Outputs:**\n",
      "    *   Explain concepts like feature importance (e.g., from SHAP or LIME) in a more understandable way.\n",
      "    *   Generate text explanations for model predictions (though the LLM itself isn't the *model*, it can help articulate *why* a model made a certain prediction if provided with the underlying data/metrics).\n",
      "*   **Synthetic Data Generation (Text-based):**\n",
      "    *   Create realistic-sounding text data for testing or augmentation, especially useful for NLP tasks.\n",
      "\n",
      "### 4. Documentation, Communication & Learning\n",
      "\n",
      "*   **Report & Presentation Generation:**\n",
      "    *   Draft sections of data analysis reports, summaries of findings, or outlines for presentations. \"Write an executive summary for a report on customer churn, highlighting key drivers.\"\n",
      "    *   Generate explanations for complex statistical or ML concepts for a non-technical audience.\n",
      "*   **Technical Writing:**\n",
      "    *   Write blog posts, tutorials, or README files for projects.\n",
      "    *   Improve clarity and conciseness of existing documentation.\n",
      "*   **Learning New Concepts:**\n",
      "    *   Act as a personal tutor: \"Explain Principal Component Analysis (PCA) simply.\"\n",
      "    *   Provide step-by-step guides for complex procedures.\n",
      "*   **Brainstorming & Ideation:**\n",
      "    *   \"What are some innovative ways to visualize high-dimensional data?\"\n",
      "    *   \"Suggest different approaches to anomaly detection in sensor data.\"\n",
      "\n",
      "### 5. Building LLM-Powered Applications\n",
      "\n",
      "*   **Natural Language Interfaces (NLIs):**\n",
      "    *   Build tools where users can query data or execute commands using natural language instead of code (e.g., \"Show me sales by region for last quarter\").\n",
      "    *   Create chatbots that can answer questions about a specific dataset or internal knowledge base.\n",
      "*   **Automated Data Storytelling:**\n",
      "    *   Generate narrative summaries from data visualizations or statistical results.\n",
      "*   **Text Analysis Pipelines:**\n",
      "    *   Use LLMs for advanced text classification, entity extraction, sentiment analysis, or summarization where pre-trained models might not be sufficient or flexible enough.\n",
      "\n",
      "### How PyData Members Can Implement This:\n",
      "\n",
      "*   **Direct Interaction:** Use public LLM interfaces (like ChatGPT, Bard, Claude) for quick queries and assistance.\n",
      "*   **API Integration:** Integrate LLM APIs (e.g., OpenAI API, Google Gemini API, Hugging Face Inference API) directly into Python scripts and applications for more automated and specific tasks.\n",
      "*   **Local/Open-Source LLMs:** For privacy-sensitive data or specific requirements, run smaller, open-source LLMs locally (e.g., Llama 2, Mistral via `ollama` or `transformers` library).\n",
      "*   **LangChain/LlamaIndex:** Use frameworks like LangChain or LlamaIndex to build complex applications that chain LLM calls, integrate with external data sources, and manage conversational memory.\n",
      "\n",
      "### Important Considerations:\n",
      "\n",
      "*   **Hallucination:** LLMs can generate factually incorrect information. Always verify critical outputs, especially code and factual data.\n",
      "*   **Data Privacy:** Be cautious about pasting sensitive or proprietary data into public LLM interfaces. Use local models or secure API integrations for such cases.\n",
      "*   **Bias:** LLMs can reflect biases present in their training data. Be aware of this when using them for sensitive tasks like content generation or decision support.\n",
      "*   **Prompt Engineering:** The quality of the output heavily depends on the clarity and specificity of the input prompt. PyData members can excel at crafting precise prompts.\n",
      "\n",
      "In summary, LLMs offer PyData members a powerful toolkit to enhance their coding, data analysis, machine learning development, and communication efforts, acting as an intelligent co-pilot across various stages of their work.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What do you think pydata members can use LLMs for?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to gemini\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6425e0",
   "metadata": {},
   "source": [
    "### Dealing with Hallucinations and Knowledge Cutoffs\n",
    "\n",
    "We have a functioning chatbot, but its knowledge is limited to what it learned during its training. We call this the model's *parametric knowledge*. This means it has no access to real-time information or very recent developments.\n",
    "\n",
    "This limitation becomes clear when we ask about something new, like the \"Kimi K2\" model. The model may either admit it doesn't know or, more problematically, \"hallucinate\" an answer that sounds plausible but is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848020c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Kimi K2?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to gemini\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8dbc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi K2, developed by Chinese AI startup **Moonshot AI**, is primarily special for its **exceptionally long context window**.\n",
      "\n",
      "Let's break down why this is a significant differentiator:\n",
      "\n",
      "1.  **Massive Context Window:**\n",
      "    *   Kimi K2 boasts a context window of **200,000 characters (or tokens, roughly equivalent to 200,000 Chinese characters or 200,000 English tokens)**.\n",
      "    *   **Why is this special?** At the time of its release and even now, this was significantly larger than many leading models. For comparison, early versions of GPT-4 had context windows of 8k or 32k tokens, and while Claude 2.1 expanded to 200k tokens, Kimi was among the first to truly push this boundary for a widely accessible model.\n",
      "    *   **What does \"context window\" mean?** It's the amount of information (text, code, data) that the LLM can \"see\" and process at any given moment to understand your query and generate a response. It's like its short-term memory and reading comprehension limit.\n",
      "\n",
      "2.  **Implications of a Long Context Window:**\n",
      "    *   **Processing Entire Documents/Books:** You can feed Kimi K2 an entire novel, a large research paper, a comprehensive financial report, or an extensive codebase, and it can understand the relationships, summarize, extract information, or answer questions spanning the entire document.\n",
      "    *   **Maintaining Coherence in Long Conversations:** It can remember the details of very long conversations, leading to more consistent and relevant interactions over extended periods.\n",
      "    *   **Complex Code Analysis:** Developers can paste entire code repositories or large files and ask for explanations, bug detection, or refactoring suggestions across the whole project.\n",
      "    *   **Deep Data Analysis:** When combined with text-based data, it can perform more sophisticated analysis by seeing more of the underlying context.\n",
      "    *   **Reduced Need for Chunking/Summarization:** Users don't have to break down large texts into smaller pieces or rely on external summarization tools before feeding them to the model.\n",
      "\n",
      "3.  **Strong Chinese Language Capabilities:**\n",
      "    *   As a model from a Chinese company, Kimi K2 naturally excels in processing, understanding, and generating Chinese language text with high fluency and accuracy. This makes it a top choice for users and applications operating primarily in Chinese.\n",
      "\n",
      "4.  **Efficiency and Speed:**\n",
      "    *   Despite handling such large contexts, Moonshot AI has focused on making Kimi K2 efficient and relatively fast, which is crucial for practical applications.\n",
      "\n",
      "5.  **Competitive Pricing:**\n",
      "    *   Moonshot AI has also positioned Kimi K2 with competitive pricing for its API access, making its powerful capabilities more accessible to developers and businesses.\n",
      "\n",
      "**In essence, Kimi K2's \"specialness\" lies in its ability to digest and reason over vast amounts of information in a single go, significantly expanding the scope and depth of tasks that an LLM can perform.** This capability has pushed the boundaries of what's expected from LLMs and has spurred other major players to also increase their context window sizes.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ead6f1",
   "metadata": {},
   "source": [
    "### Augmenting with Source Knowledge\n",
    "\n",
    "The chatbot's answer is likely generic or incorrect because \"Kimi K2\" is too new for its training data. We can solve this by providing the necessary information directly in the prompt. This is called *source knowledge*.\n",
    "\n",
    "Let's define a block of text containing key facts about Kimi K2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38e1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_knowledge = (\n",
    "\"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters.\",\n",
    "\"Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\",\n",
    "\"It was pre-trained on 15.5T tokens with zero training instability.\",\n",
    "\"The model is specifically designed for tool use, reasoning, and autonomous problem-solving.\",\n",
    "\"Kimi K2 comes in two variants: Kimi-K2-Base, the foundation model for custom solutions, and Kimi-K2-Instruct, a post-trained model for general-purpose chat and agentic experiences.\",\n",
    "\"Its architecture is a Mixture-of-Experts (MoE) with 1 trillion total parameters and 32 billion activated parameters.\",\n",
    "\"The model has 61 layers, including 1 dense layer, an attention hidden dimension of 7168, and a MoE hidden dimension of 2048 per expert.\",\n",
    "\"It features 384 experts with 8 selected per token, 1 shared expert, and 64 attention heads.\",\n",
    "\"The model has a vocabulary size of 160K, a context length of 128K, and uses MLA for its attention mechanism and SwiGLU as its activation function.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2896e",
   "metadata": {},
   "source": [
    "Now, we can create an *augmented prompt* that includes instructions for the model, our source knowledge, and the original query. This tells the LLM to base its answer on the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e54a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is so special about Kimi K2?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58bdcc",
   "metadata": {},
   "source": [
    "Now, let's pass this new, augmented prompt to the model. Notice that we are creating a new `HumanMessage` with this detailed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a25656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to gemini\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04b2395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi K2 is special for several reasons, primarily due to its advanced architecture and optimization for specific capabilities:\n",
      "\n",
      "1.  **State-of-the-Art Mixture-of-Experts (MoE) Architecture:** It's an MoE model with a massive scale, featuring 1 trillion total parameters and 32 billion activated parameters. This allows for efficiency while maintaining high capability.\n",
      "2.  **Exceptional Performance:** It achieves outstanding results across frontier knowledge, reasoning, and coding tasks.\n",
      "3.  **Optimized for Agentic Capabilities:** Kimi K2 is meticulously optimized and specifically designed for tool use, reasoning, and autonomous problem-solving, making it highly suitable for agent-based AI applications.\n",
      "4.  **Massive and Stable Training:** It was pre-trained on an enormous 15.5 trillion tokens with zero training instability, indicating a robust and high-quality training process.\n",
      "5.  **Large Context Length:** It boasts a significant context length of 128K, enabling it to process and understand very long inputs.\n",
      "6.  **Advanced Technical Features:** It utilizes the Muon optimizer, MLA for its attention mechanism, SwiGLU as its activation function, and has a high number of experts (384 with 8 selected per token) and attention heads (64).\n",
      "7.  **Specialized Variants:** It comes in two versions: Kimi-K2-Base for custom solutions and Kimi-K2-Instruct for general-purpose chat and agentic experiences.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36e7d5",
   "metadata": {},
   "source": [
    "The quality of this answer is significantly better because it's grounded in the facts we provided. This demonstrates the core idea of **Retrieval-Augmented Generation**. The main challenge is: how do we find and provide this context automatically? The answer is to build a knowledge base with a vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2056eebc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4543a9",
   "metadata": {},
   "source": [
    "### Building the Knowledge Base\n",
    "\n",
    "To automate the retrieval of source knowledge, we need to create a searchable knowledge base. This involves two key components:\n",
    "\n",
    "1.  **An Embedding Model**: This model converts text into numerical vectors (embeddings), capturing its semantic meaning.\n",
    "2.  **A Vector Store**: This is a specialized database designed to store and efficiently search these vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48dc04",
   "metadata": {},
   "source": [
    "First, we'll set up our embedding model. We're using a popular, high-performance model from Hugging Face called `all-mpnet-base-v2`. The `HuggingFaceEmbeddings` class from LangChain makes it easy to use locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2626881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abuzarsultan/workspace/github.com/AbzyS1/pydata-rag/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908fe30d",
   "metadata": {},
   "source": [
    "Next, we initialize our vector store. We'll use **Chroma**, a lightweight, open-source and fast vector database that can run entirely in-memory or be persisted to disk. We'll give our collection a name and tell it which embedding function to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87146ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99ed1e",
   "metadata": {},
   "source": [
    "Now it's time to populate our knowledge base. Instead of a small text snippet, we'll use an entire [PDF document](https://arxiv.org/pdf/2501.17805). The process involves three steps:\n",
    "\n",
    "1.  **Load**: Use `PyPDFLoader` to load the content of the PDF.\n",
    "2.  **Split**: Large documents are too big to fit into the context window of many models. We use `RecursiveCharacterTextSplitter` to break the document into smaller, overlapping chunks. This ensures semantic context is preserved at the boundaries of each chunk.\n",
    "3.  **Add**: Add the processed chunks to our vector store. Chroma will automatically use our embedding model to convert each chunk into a vector and index it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60309e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded, split, and added the PDF to the vector store.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Load the PDF\n",
    "loader = PyPDFLoader(\"pdfs/2501.17805v1.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Split the PDF into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Add the chunks to the vector store\n",
    "vector_store.add_documents(docs)\n",
    " \n",
    "print(\"Successfully loaded, split, and added the PDF to the vector store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac27365",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation\n",
    "\n",
    "Our knowledge base is now ready. We can perform a similarity search to find the document chunks most relevant to a given query. This search works by embedding the query and finding the vectors in our database that are closest to it in the high-dimensional space.\n",
    "\n",
    "Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae25e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** and ensure various risk activities \n",
      "(i.e. all of the above) are \n",
      "cohesively structured and \n",
      "aligned, risk roles and \n",
      "responsibilities are clearly \n",
      "defined, and checks and \n",
      "balances are in place to avoid \n",
      "silos and manage conflicts of \n",
      "interest. \n",
      "In other safety critical \n",
      "industries, the Three Lines of \n",
      "Defence framework – \n",
      "separating risk ownership, \n",
      "oversight and audit – is \n",
      "widely used and can be \n",
      "usefully applied to advanced \n",
      "AI companies (954, 955) \n",
      " \n",
      "Table 3.1: Several practices and mechanisms, organised by five stages of risk management, can help manage the broad \n",
      "range of risks posed by general-purpose AI. \n",
      " \n",
      "Documentation and institutional transparency mechanisms, together with information sharing \n",
      "practices, play an important role in managing the risks of general-purpose AI and facilitating \n",
      "external scrutiny. It has become common practice to test models before release, including via \n",
      "\n",
      "*** performance. \n",
      " \n",
      "This section covers six general technical challenges that can make risk management and \n",
      "policymaking more difficult for a wide range of general-purpose AI risks (see Figure 3.1).  \n",
      " \n",
      "A. Autonomous general-purpose AI agents may increase risks: general-purpose AI agents – \n",
      "systems that can plan and act in the world with little to no human involvement elevate risks of \n",
      "malfunctions and malicious use. Today, general-purpose AI systems are primarily used as tools by \n",
      "humans. For example, a chatbot can write computer code, but a human runs, debugs, and \n",
      "integrates code into a larger software project. However, researchers and developers are making \n",
      "large efforts to design general-purpose AI agents – systems that can act and plan autonomously \n",
      "by controlling computers, programming interfaces, robotic tools, and by delegating to other AI \n",
      "systems (18, 55, 316*, 984, 985, 986*, 987, 988, 989, 990, 991*, 992). These systems are also \n",
      "\n",
      "*** Technical approaches to risk management \n",
      "                          3.1 Risk management overview \n",
      " \n",
      "158 \n",
      "3.1. Risk management overview \n",
      " \n",
      "KEY INFORMATION \n",
      " \n",
      "● Risk management – identifying and assessing risks, and then mitigating and monitoring \n",
      "them – is challenging in the context of general-purpose AI. While numerous frameworks \n",
      "and practices are under development globally, significant gaps remain in validation, \n",
      "standardisation, and implementation across sectors and jurisdictions, particularly for \n",
      "identifying and mitigating unprecedented risks. \n",
      "● The context of general-purpose AI risk management is uniquely complex due to the \n",
      "technology’s rapid evolution and broad applicability. Traditional risk management \n",
      "practices (such as safety by design, audits, redundancy, and safety cases) provide a \n",
      "foundation, but must be adapted given the rapid evolution, broad applicability, and \n",
      "complex interaction effects of general-purpose AI. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What are 5 points from the report on what makes risk management for general-purpose AI particularly difficult?\"\n",
    "\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "for res in results:\n",
    "    print(f\"*** {res.page_content} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072da5bf",
   "metadata": {},
   "source": [
    "The search successfully retrieved relevant passages from the PDF. Now we can automate the process we performed manually earlier. We'll create a function `augment_prompt` that:\n",
    "1.  Takes a user's query.\n",
    "2.  Performs a similarity search on our `vector_store`.\n",
    "3.  Joins the content of the retrieved chunks into a single context block.\n",
    "4.  Constructs a detailed prompt for the LLM, including instructions, the retrieved context, and the original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 5 results from knowledge base\n",
    "    results = vector_store.similarity_search(query, k=5)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "    ---\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "    ---\n",
    "    Query: {query}\"\"\"\n",
    "\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373ce25",
   "metadata": {},
   "source": [
    "Let's see what the final, augmented prompt looks like before sending it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db6e3a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "    ---\n",
      "    Contexts:\n",
      "    and ensure various risk activities \n",
      "(i.e. all of the above) are \n",
      "cohesively structured and \n",
      "aligned, risk roles and \n",
      "responsibilities are clearly \n",
      "defined, and checks and \n",
      "balances are in place to avoid \n",
      "silos and manage conflicts of \n",
      "interest. \n",
      "In other safety critical \n",
      "industries, the Three Lines of \n",
      "Defence framework – \n",
      "separating risk ownership, \n",
      "oversight and audit – is \n",
      "widely used and can be \n",
      "usefully applied to advanced \n",
      "AI companies (954, 955) \n",
      " \n",
      "Table 3.1: Several practices and mechanisms, organised by five stages of risk management, can help manage the broad \n",
      "range of risks posed by general-purpose AI. \n",
      " \n",
      "Documentation and institutional transparency mechanisms, together with information sharing \n",
      "practices, play an important role in managing the risks of general-purpose AI and facilitating \n",
      "external scrutiny. It has become common practice to test models before release, including via\n",
      "performance. \n",
      " \n",
      "This section covers six general technical challenges that can make risk management and \n",
      "policymaking more difficult for a wide range of general-purpose AI risks (see Figure 3.1).  \n",
      " \n",
      "A. Autonomous general-purpose AI agents may increase risks: general-purpose AI agents – \n",
      "systems that can plan and act in the world with little to no human involvement elevate risks of \n",
      "malfunctions and malicious use. Today, general-purpose AI systems are primarily used as tools by \n",
      "humans. For example, a chatbot can write computer code, but a human runs, debugs, and \n",
      "integrates code into a larger software project. However, researchers and developers are making \n",
      "large efforts to design general-purpose AI agents – systems that can act and plan autonomously \n",
      "by controlling computers, programming interfaces, robotic tools, and by delegating to other AI \n",
      "systems (18, 55, 316*, 984, 985, 986*, 987, 988, 989, 990, 991*, 992). These systems are also\n",
      "Technical approaches to risk management \n",
      "                          3.1 Risk management overview \n",
      " \n",
      "158 \n",
      "3.1. Risk management overview \n",
      " \n",
      "KEY INFORMATION \n",
      " \n",
      "● Risk management – identifying and assessing risks, and then mitigating and monitoring \n",
      "them – is challenging in the context of general-purpose AI. While numerous frameworks \n",
      "and practices are under development globally, significant gaps remain in validation, \n",
      "standardisation, and implementation across sectors and jurisdictions, particularly for \n",
      "identifying and mitigating unprecedented risks. \n",
      "● The context of general-purpose AI risk management is uniquely complex due to the \n",
      "technology’s rapid evolution and broad applicability. Traditional risk management \n",
      "practices (such as safety by design, audits, redundancy, and safety cases) provide a \n",
      "foundation, but must be adapted given the rapid evolution, broad applicability, and \n",
      "complex interaction effects of general-purpose AI.\n",
      "implement in the face of various power dynamics (932). \n",
      " \n",
      "Risk management mechanisms and practices  \n",
      " \n",
      "There are numerous practices and mechanisms that can help manage the broad range of risks \n",
      "posed by general-purpose AI. Some of these are referenced in Table 3.1 below; they are discussed \n",
      "throughout 3. Technical approaches to risk management in more detail. \n",
      " \n",
      "Table 3.1 below includes risk management practices that support five (interconnected) stages of \n",
      "risk management:  \n",
      " \n",
      "● Risk identification: The process of finding, recognising, and describing risks. \n",
      "● Risk assessment: The process to understand the nature of risk and to determine the level of \n",
      "risk. \n",
      "● Risk evaluation: The process of comparing the results of risk assessment with risk criteria to \n",
      "determine whether the risk and/or its magnitude is/are acceptable or tolerable. (Note that \n",
      "the term ‘evaluation’ has multiple meanings in the context of AI and can also refer to testing \n",
      "models.)\n",
      "capabilities and risks of advanced AI. \n",
      " \n",
      "The report is organised into five main sections: After this Introduction, 1. Capabilities of \n",
      "general-purpose AI provides information on the current capabilities of general-purpose AI, \n",
      "underlying principles, and potential future trends. 2. Risks discusses risks associated with \n",
      "general-purpose AI. 3. Technical approaches to risk management presents technical approaches to \n",
      "mitigating risks from general-purpose AI and evaluates their strengths and limitations. The \n",
      "Conclusion summarises and concludes.\n",
      "    ---\n",
      "    Query: What are 5 points from the report on what makes risk management for general-purpose AI particularly difficult?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124381f",
   "metadata": {},
   "source": [
    "This prompt now contains everything the model needs to generate a high-quality, fact-based answer. We'll create a new, clean message list for this RAG-powered query to avoid confusing the model with our previous, non-RAG conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac524710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided contexts, here are 5 points that make risk management for general-purpose AI particularly difficult:\n",
      "\n",
      "1.  **Significant Gaps in Validation, Standardization, and Implementation:** There are considerable gaps globally in validating, standardizing, and implementing risk management frameworks and practices, especially for identifying and mitigating unprecedented risks.\n",
      "2.  **Rapid Evolution of Technology:** The technology's rapid evolution makes the context of general-purpose AI risk management uniquely complex.\n",
      "3.  **Broad Applicability of Technology:** The broad applicability of general-purpose AI contributes to the unique complexity of its risk management.\n",
      "4.  **Complex Interaction Effects:** General-purpose AI systems exhibit complex interaction effects, which traditional risk management practices must be adapted to address.\n",
      "5.  **Emergence of Autonomous Agents:** Autonomous general-purpose AI agents, which can plan and act with little to no human involvement, elevate the risks of malfunctions and malicious use, making risk management more challenging.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "\n",
    "rag_messages = [SystemMessage(content=\"You are a helpful assistant.\"),]\n",
    "# add to messages\n",
    "rag_messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(rag_messages)\n",
    "\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
